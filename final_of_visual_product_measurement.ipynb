{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vqQEGR_jqP2J"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers pandas openpyxl pillow requests reportlab opencv-python\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import requests\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "import clip\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n"
      ],
      "metadata": {
        "id": "o4RUjaxsrHEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ],
      "metadata": {
        "id": "rGPKyxVSrbcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "blip_model = BlipForConditionalGeneration.from_pretrained(\n",
        "    \"Salesforce/blip-image-captioning-base\"\n",
        ").to(device)\n",
        "blip_model.eval()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "zQV9EywLrrD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "clip_model.eval()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yc5bUXMzrvxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()  # upload A1.0_data_product_images.xlsx\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZJ83EERxsh7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel(\"A1.0_data_product_images.xlsx\")\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "image_columns = [c for c in df.columns if c.startswith(\"Image\") and c != \"Image Count\"]\n"
      ],
      "metadata": {
        "id": "d9cGqMcAsnzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_url(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=5)\n",
        "        r.raise_for_status()\n",
        "        return Image.open(BytesIO(r.content)).convert(\"RGB\")\n",
        "    except:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "EyvKbYpfswXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_image_with_blip(image: Image.Image) -> str:\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        out = blip_model.generate(**inputs, max_new_tokens=30)\n",
        "    return processor.decode(out[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "JxWW9DUVs3ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_basic_visual_features(image: Image.Image):\n",
        "    img = np.array(image)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    edges = cv2.Canny(gray, 50, 150)\n",
        "    edge_density = edges.mean()\n",
        "\n",
        "    brightness = gray.mean()\n",
        "\n",
        "    h, w = gray.shape\n",
        "    aspect_ratio = w / h if h > 0 else 1.0\n",
        "\n",
        "    return {\n",
        "        \"edge_density\": edge_density,\n",
        "        \"brightness\": brightness,\n",
        "        \"aspect_ratio\": aspect_ratio\n",
        "    }\n"
      ],
      "metadata": {
        "id": "zcyv5ixUs6or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_similarity(image, prompts):\n",
        "    image_input = clip_preprocess(image).unsqueeze(0).to(device)\n",
        "    text_inputs = clip.tokenize(prompts).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_feat = clip_model.encode_image(image_input)\n",
        "        text_feat = clip_model.encode_text(text_inputs)\n",
        "\n",
        "        image_feat /= image_feat.norm(dim=-1, keepdim=True)\n",
        "        text_feat /= text_feat.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        sims = (image_feat @ text_feat.T).squeeze(0)\n",
        "\n",
        "    return {prompts[i]: sims[i].item() for i in range(len(prompts))}\n"
      ],
      "metadata": {
        "id": "aCI2rtvNs_mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_to_range(value, in_min=-0.05, in_max=0.05):\n",
        "    value = max(in_min, min(in_max, value))\n",
        "    norm = (value - in_min) / (in_max - in_min)\n",
        "    return round(-5 + norm * 10, 2)\n",
        "\n",
        "\n",
        "def infer_gender_expression_with_clip(image):\n",
        "    scores = clip_similarity(\n",
        "        image,\n",
        "        [\"masculine eyeglasses\", \"unisex eyeglasses\", \"feminine eyeglasses\"]\n",
        "    )\n",
        "    raw = scores[\"feminine eyeglasses\"] - scores[\"masculine eyeglasses\"]\n",
        "    return scale_to_range(raw)\n",
        "\n",
        "\n",
        "def infer_dominant_color_with_clip(image):\n",
        "    scores = clip_similarity(\n",
        "        image,\n",
        "        [\"black eyeglasses\", \"brown eyeglasses\", \"clear eyeglasses\", \"colored eyeglasses\"]\n",
        "    )\n",
        "    return max(scores, key=scores.get).split()[0]\n",
        "\n",
        "\n",
        "def infer_visual_weight_with_clip(image):\n",
        "    scores = clip_similarity(\n",
        "        image,\n",
        "        [\"thin lightweight eyeglasses\", \"bold heavy eyeglasses\"]\n",
        "    )\n",
        "    raw = scores[\"bold heavy eyeglasses\"] - scores[\"thin lightweight eyeglasses\"]\n",
        "    return scale_to_range(raw)\n",
        "\n",
        "\n",
        "def infer_embellishment_with_clip(image):\n",
        "    scores = clip_similarity(\n",
        "        image,\n",
        "        [\"simple minimalist eyeglasses\", \"ornate decorative eyeglasses\"]\n",
        "    )\n",
        "    raw = scores[\"ornate decorative eyeglasses\"] - scores[\"simple minimalist eyeglasses\"]\n",
        "    return scale_to_range(raw)\n"
      ],
      "metadata": {
        "id": "iKFM-ArztAsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_transparency(brightness):\n",
        "    if brightness > 180:\n",
        "        return \"transparent\"\n",
        "    elif brightness > 130:\n",
        "        return \"semi-transparent\"\n",
        "    else:\n",
        "        return \"opaque\"\n"
      ],
      "metadata": {
        "id": "eymcc83TtE0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_frame_geometry_with_clip(image, aspect_ratio):\n",
        "    scores = clip_similarity(\n",
        "        image,\n",
        "        [\n",
        "            \"round eyeglasses\",\n",
        "            \"rectangular eyeglasses\",\n",
        "            \"square eyeglasses\",\n",
        "            \"cat-eye eyeglasses\"\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Sort by similarity\n",
        "    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_label, top_score = sorted_scores[0]\n",
        "    second_label, second_score = sorted_scores[1]\n",
        "\n",
        "    # 1️⃣ Strong CLIP confidence → trust CLIP\n",
        "    if top_score - second_score > 0.015:\n",
        "        return top_label.split()[0]\n",
        "\n",
        "    # 2️⃣ Weak CLIP signal → use visual geometry fallback\n",
        "    if aspect_ratio >= 1.4:\n",
        "        return \"rectangular\"\n",
        "    elif aspect_ratio <= 1.15:\n",
        "        return \"round\"\n",
        "\n",
        "    # 3️⃣ Only now call it ambiguous\n",
        "    return \"ambiguous\"\n",
        "\n"
      ],
      "metadata": {
        "id": "uR2JmCXJtH7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisualScores(BaseModel):\n",
        "    gender_expression: float\n",
        "    visual_weight: float\n",
        "    embellishment: float\n",
        "    unconventionality: float\n",
        "    formality: float\n",
        "\n",
        "\n",
        "class VisualAttributes(BaseModel):\n",
        "    wirecore: bool\n",
        "    frame_geometry: str\n",
        "    transparency: str\n",
        "    dominant_colors: List[str]\n",
        "    textures: List[str]\n",
        "    suitable_for_kids: bool\n",
        "\n",
        "\n",
        "class ProductAnalysis(BaseModel):\n",
        "    scores: VisualScores\n",
        "    attributes: VisualAttributes\n",
        "    confidence: float\n",
        "    notes: str\n"
      ],
      "metadata": {
        "id": "n4K45BFNtMRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def caption_to_measurements(\n",
        "    image,\n",
        "    caption,\n",
        "    product_id,\n",
        "    frame_geometry,\n",
        "    dominant_color,\n",
        "    texture,\n",
        "    brightness,\n",
        "    gender_expression,\n",
        "    transparency,\n",
        "    visual_weight,\n",
        "    embellishment\n",
        "):\n",
        "    formality = 1.0 if \"metal\" in caption.lower() else 0.0\n",
        "    unconventionality = 1.0 if frame_geometry in [\"cat-eye\", \"ambiguous\"] else 0.0\n",
        "\n",
        "    scores = VisualScores(\n",
        "        gender_expression=gender_expression,\n",
        "        visual_weight=visual_weight,\n",
        "        embellishment=embellishment,\n",
        "        unconventionality=unconventionality,\n",
        "        formality=formality\n",
        "    )\n",
        "\n",
        "    attributes = VisualAttributes(\n",
        "        wirecore=\"metal\" in caption.lower(),\n",
        "        frame_geometry=frame_geometry,\n",
        "        transparency=transparency,\n",
        "        dominant_colors=[dominant_color],\n",
        "        textures=[texture],\n",
        "        suitable_for_kids=(\n",
        "            visual_weight <= 0 and embellishment < 0 and frame_geometry != \"cat-eye\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "    return ProductAnalysis(\n",
        "        scores=scores,\n",
        "        attributes=attributes,\n",
        "        confidence=0.85,\n",
        "        notes=\"Derived exclusively from observable visual characteristics.\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "dCArcp8_tOpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def most_frequent_caption(captions, top_k=12):\n",
        "    \"\"\"\n",
        "    Builds a representative caption using the most frequent\n",
        "    meaningful words across all BLIP captions.\n",
        "    \"\"\"\n",
        "\n",
        "    stopwords = {\n",
        "        \"a\", \"the\", \"and\", \"of\", \"with\", \"on\", \"in\", \"is\", \"are\",\n",
        "        \"this\", \"that\", \"pair\", \"glasses\", \"eyeglasses\", \"frame\", \"frames\"\n",
        "    }\n",
        "\n",
        "    words = []\n",
        "    for c in captions:\n",
        "        tokens = re.findall(r\"[a-zA-Z]+\", c.lower())\n",
        "        words.extend([t for t in tokens if t not in stopwords])\n",
        "\n",
        "    if not words:\n",
        "        return captions[0]\n",
        "\n",
        "    most_common = [w for w, _ in Counter(words).most_common(top_k)]\n",
        "\n",
        "    return \" \".join(most_common)\n"
      ],
      "metadata": {
        "id": "aU-gsja8wAlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "MAX_PRODUCTS = 10\n",
        "\n",
        "for _, row in df.head(MAX_PRODUCTS).iterrows():\n",
        "    product_id = row[\"Product Id\"]\n",
        "    print(f\"\\nStarting product {product_id}\")\n",
        "\n",
        "    # ---- Load images safely ----\n",
        "    urls = [row[c] for c in image_columns if isinstance(row[c], str)]\n",
        "    images = []\n",
        "    for u in urls:\n",
        "        img = load_image_from_url(u)\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "\n",
        "    if not images:\n",
        "        print(\"No valid images, skipping\")\n",
        "        continue\n",
        "\n",
        "    captions = []\n",
        "    genders = []\n",
        "    weights = []\n",
        "    embellishments = []\n",
        "    geometries = []\n",
        "    colors = []\n",
        "    transparencies = []\n",
        "    textures = []\n",
        "    brightness_vals = []\n",
        "\n",
        "    # ---- Per-image processing ----\n",
        "    for image in images:\n",
        "        caption = analyze_image_with_blip(image)\n",
        "        features = extract_basic_visual_features(image)\n",
        "\n",
        "        captions.append(caption)\n",
        "        genders.append(infer_gender_expression_with_clip(image))\n",
        "        weights.append(infer_visual_weight_with_clip(image))\n",
        "        embellishments.append(infer_embellishment_with_clip(image))\n",
        "        geometries.append(\n",
        "            infer_frame_geometry_with_clip(image, features[\"aspect_ratio\"])\n",
        "        )\n",
        "        colors.append(infer_dominant_color_with_clip(image))\n",
        "        transparencies.append(infer_transparency(features[\"brightness\"]))\n",
        "        textures.append(\"smooth\" if features[\"edge_density\"] < 30 else \"textured\")\n",
        "        brightness_vals.append(features[\"brightness\"])\n",
        "\n",
        "    # ---- BUILD REPRESENTATIVE CAPTION (KEY CHANGE) ----\n",
        "    representative_caption = most_frequent_caption(captions)\n",
        "    print(\"BLIP Caption (Most Frequent Words):\", representative_caption)\n",
        "\n",
        "    # ---- Aggregate measurements ----\n",
        "    analysis = caption_to_measurements(\n",
        "        image=images[0],\n",
        "        caption=representative_caption,\n",
        "        product_id=product_id,\n",
        "        frame_geometry=max(set(geometries), key=geometries.count),\n",
        "        dominant_color=max(set(colors), key=colors.count),\n",
        "        texture=max(set(textures), key=textures.count),\n",
        "        brightness=sum(brightness_vals) / len(brightness_vals),\n",
        "        gender_expression=sum(genders) / len(genders),\n",
        "        transparency=max(set(transparencies), key=transparencies.count),\n",
        "        visual_weight=sum(weights) / len(weights),\n",
        "        embellishment=sum(embellishments) / len(embellishments)\n",
        "    )\n",
        "\n",
        "    results.append({\n",
        "        \"product_id\": product_id,\n",
        "        \"image_count\": len(images),\n",
        "        \"visual_description\": representative_caption,\n",
        "        \"analysis\": analysis.model_dump()\n",
        "    })\n",
        "\n",
        "    print(f\"Processed {product_id} | images={len(images)}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Hve2NvcStQ8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n========== FINAL JSON OUTPUT ==========\\n\")\n",
        "print(json.dumps(results, indent=2))\n",
        "\n"
      ],
      "metadata": {
        "id": "drTsfFuZtTRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from reportlab.platypus import (\n",
        "    SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle\n",
        ")\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib import colors\n",
        "\n",
        "\n",
        "def generate_stylish_pdf(results, filename=\"visual_product_report.pdf\"):\n",
        "    doc = SimpleDocTemplate(\n",
        "        filename,\n",
        "        pagesize=A4,\n",
        "        rightMargin=36,\n",
        "        leftMargin=36,\n",
        "        topMargin=36,\n",
        "        bottomMargin=36,\n",
        "    )\n",
        "\n",
        "    styles = getSampleStyleSheet()\n",
        "    styles.add(\n",
        "        ParagraphStyle(\n",
        "            name=\"SectionHeader\",\n",
        "            fontSize=13,\n",
        "            spaceAfter=8,\n",
        "            textColor=colors.darkblue,\n",
        "        )\n",
        "    )\n",
        "\n",
        "    elements = []\n",
        "\n",
        "    # ---- Title ----\n",
        "    elements.append(Paragraph(\n",
        "        \"<b>Visual Product Measurement Report</b>\",\n",
        "        styles[\"Title\"]\n",
        "    ))\n",
        "    elements.append(Spacer(1, 20))\n",
        "\n",
        "    for r in results:\n",
        "        # ---- Product Header ----\n",
        "        elements.append(Paragraph(\n",
        "            f\"<b>Product ID:</b> {r['product_id']}\",\n",
        "            styles[\"Heading2\"]\n",
        "        ))\n",
        "        elements.append(Paragraph(\n",
        "            f\"<b>Image Count:</b> {r['image_count']}\",\n",
        "            styles[\"Normal\"]\n",
        "        ))\n",
        "        elements.append(Spacer(1, 8))\n",
        "\n",
        "        # ---- Caption ----\n",
        "        elements.append(Paragraph(\"Visual Description\", styles[\"SectionHeader\"]))\n",
        "        elements.append(Paragraph(\n",
        "            r[\"visual_description\"],\n",
        "            styles[\"BodyText\"]\n",
        "        ))\n",
        "        elements.append(Spacer(1, 10))\n",
        "\n",
        "        # ---- Scores Table ----\n",
        "        elements.append(Paragraph(\"Visual Measurements\", styles[\"SectionHeader\"]))\n",
        "        scores = r[\"analysis\"][\"scores\"]\n",
        "        score_rows = [[\"Dimension\", \"Score (-5 → +5)\"]] + [\n",
        "            [k.replace(\"_\", \" \").title(), f\"{v:.2f}\"]\n",
        "            for k, v in scores.items()\n",
        "        ]\n",
        "\n",
        "        score_table = Table(score_rows, colWidths=[240, 120])\n",
        "        score_table.setStyle(TableStyle([\n",
        "            (\"BACKGROUND\", (0, 0), (-1, 0), colors.whitesmoke),\n",
        "            (\"GRID\", (0, 0), (-1, -1), 0.8, colors.grey),\n",
        "            (\"FONT\", (0, 0), (-1, 0), \"Helvetica-Bold\"),\n",
        "            (\"ALIGN\", (1, 1), (-1, -1), \"CENTER\"),\n",
        "        ]))\n",
        "\n",
        "        elements.append(score_table)\n",
        "        elements.append(Spacer(1, 12))\n",
        "\n",
        "        # ---- Attributes ----\n",
        "        elements.append(Paragraph(\"Observable Attributes\", styles[\"SectionHeader\"]))\n",
        "        attrs = r[\"analysis\"][\"attributes\"]\n",
        "        for k, v in attrs.items():\n",
        "            elements.append(\n",
        "                Paragraph(f\"<b>{k.replace('_', ' ').title()}:</b> {v}\", styles[\"Normal\"])\n",
        "            )\n",
        "\n",
        "        elements.append(Spacer(1, 18))\n",
        "\n",
        "    doc.build(elements)\n",
        "    print(f\"PDF generated: {filename}\")\n"
      ],
      "metadata": {
        "id": "YMGs9uey0VHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_stylish_pdf(results)\n",
        "from google.colab import files\n",
        "files.download(\"visual_product_report.pdf\")\n"
      ],
      "metadata": {
        "id": "JCxmvoE-0ikt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zBEW1MYT0wqA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}